import os
import queue
import threading
import logging
from datetime import datetime
from uuid import uuid4

import numpy as np
import torch
from scipy.io.wavfile import write
import requests
import soundfile as sf

from config.config import (
    STREAM_SAMPLE_RATE,
    MEET_FRAME_DURATION_MS,
    MEET_AUDIO_CHUNKS_DIR,
    SUMMARY_OUTPUT_DIR,
    SILENCE_THRESHOLD_FRAMES,
)

from handlers.llm_handler import get_mary_response, get_summary_response, get_title_response
from handlers.diarization_handler import run_diarization, process_rttm_and_transcribe
from api.utils import combine_audio_chunks
from config.load_models import create_new_vad_model, asr_model

logger = logging.getLogger(__name__)

class WebsiteListenerBot:

     # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∫–ª–∞—Å—Å–∞
    def __init__(self, session_id: str, meeting_id: int):

        self.session_id = session_id # ID –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–µ—Å—Å–∏–∏
        self.meeting_id = meeting_id # ID –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–µ—Å—Å–∏–∏
        self.audio_queue = queue.Queue() # –î–ª—è –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫–∞

        self.is_running = threading.Event()
        self.is_running.set()

        self.vad = create_new_vad_model()
        self.asr_model = asr_model # Whisper (from config.load_models import asr_model)

        self.frame_size = int(STREAM_SAMPLE_RATE * MEET_FRAME_DURATION_MS / 1000) # –î–ª—è VAD-–º–æ–¥–µ–ª–∏ (–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞)
        self.silent_frames_threshold = SILENCE_THRESHOLD_FRAMES # –ü–∞—É–∑–∞ –≤ —Ä–µ—á–∏ –≤ —Å–µ–∫.

        self.global_offset = 0.0
        self.all_segments = []

        self.output_dir = MEET_AUDIO_CHUNKS_DIR / self.session_id
        os.makedirs(self.output_dir, exist_ok=True)
        logger.info(f"[{self.session_id}] –ê—É–¥–∏–æ—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å–µ—Å—Å–∏–∏ (meet_id: {self.meeting_id}) –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –≤: '{self.output_dir}'")

        self.processor_thread = threading.Thread(target=self._process_audio_stream)
        self.processor_thread.daemon = True
        self.processor_thread.start()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —á–∞–Ω–∫
    def feed_audio_chunk(self, chunk: bytes):
        if self.is_running.is_set():
            self.audio_queue.put(chunk)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫–∞ -- —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è -- –æ—Ç–≤–µ—Ç (–µ—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω —Ç—Ä–∏–≥–≥–µ—Ä)
    def _process_audio_stream(self):
        threading.current_thread().name = f'VADProcessor-{self.meeting_id}'
        logger.info(f"[{self.meeting_id}] –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä VAD –∑–∞–ø—É—â–µ–Ω —Å –º–æ–¥–µ–ª—å—é Silero.")

        vad_buffer = None
        VAD_CHUNK_SIZE = 512
        speech_buffer_for_asr = []
        is_speaking = False
        silent_frames_after_speech = 0

        while self.is_running.is_set():
            try:

                audio_frame_bytes = self.audio_queue.get(timeout=1)
                if not audio_frame_bytes:
                    continue

                audio_np = np.frombuffer(audio_frame_bytes, dtype=np.int16).astype(np.float32) / 32768.0
                new_audio_tensor = torch.from_numpy(audio_np)

                if vad_buffer is None:
                    vad_buffer = new_audio_tensor
                else:
                    vad_buffer = torch.cat([vad_buffer, new_audio_tensor])

                while vad_buffer is not None and vad_buffer.shape[0] >= VAD_CHUNK_SIZE:
                    
                    chunk_to_process = vad_buffer[:VAD_CHUNK_SIZE]
                    vad_buffer = vad_buffer[VAD_CHUNK_SIZE:]
                    speech_prob = self.vad(chunk_to_process, STREAM_SAMPLE_RATE).item()
                    
                    if speech_prob > 0.3:
                        if not is_speaking:
                            logger.info(f"[{self.meeting_id}] –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–∞—á–∞–ª–æ —Ä–µ—á–∏.")
                            is_speaking = True
                        
                        speech_buffer_for_asr.append(chunk_to_process.numpy())
                        silent_frames_after_speech = 0
                    else:
                        if is_speaking:
                            
                            silent_frames_after_speech += 1
                            
                            if silent_frames_after_speech > self.silent_frames_threshold:
                                logger.info(f"[{self.meeting_id}] –û–±–Ω–∞—Ä—É–∂–µ–Ω –∫–æ–Ω–µ—Ü —Ñ—Ä–∞–∑—ã.")
                                is_speaking = False
                                silent_frames_after_speech = 0
                                
                                if speech_buffer_for_asr:
                                    full_audio_np = np.concatenate(speech_buffer_for_asr)
                                    speech_buffer_for_asr = []
                                    
                                    self._save_chunk(full_audio_np)

                                    segments, _ = self.asr_model.transcribe(full_audio_np, beam_size=1, best_of=1, condition_on_previous_text=False, vad_filter=False, language="ru")

                                    transcription = "".join([seg.text for seg in segments]).strip()

                                    segments_l = list(segments)

                                    for seg in segments_l:
                                        segment_data = {
                                            "start": round(self.global_offset + seg.start, 2),
                                            "end": round(self.global_offset + seg.end, 2),
                                            "text": seg.text.strip()
                                        }
                                        self.all_segments.append(segment_data)
                                        print("–î–æ–±–∞–≤–ª–µ–Ω —Å–µ–≥–º–µ–Ω—Ç:", segment_data)

                                    chunk_duration = len(full_audio_np) / 16000.0
                                    self.global_offset += chunk_duration

                                    print(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {transcription}")
                
            except queue.Empty:
                if is_speaking and speech_buffer_for_asr:
                    logger.info(f"[{self.meeting_id}] –¢–∞–π–º-–∞—É—Ç, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à—É—é—Å—è —Ä–µ—á—å.")
                    is_speaking = False
                continue
            except Exception as e:
                logger.error(f"[{self.meeting_id}] –û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ VAD: {e}", exc_info=True)

    # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ—á–∞–Ω–∫–æ–≤ -- –∑–∞–ø—É—Å–∫ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–µ–π -- —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è -- –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ -- –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –≤–Ω–µ—à–Ω–∏–π —Å–µ—Ä–≤–µ—Ä
    def _perform_post_processing(self):

        threading.current_thread().name = f'PostProcessor-{self.meeting_id}'
        logger.info(f"[{self.meeting_id}] –ù–∞—á–∏–Ω–∞—é –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É...")

        try:
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ —á–∞–Ω–∫–æ–≤
            combined_audio_filename = f"combined_meeting_{self.meeting_id}.wav"
            combined_audio_filepath = self.output_dir / combined_audio_filename

            combine_audio_chunks(
                output_dir=self.output_dir,
                stream_sample_rate=STREAM_SAMPLE_RATE,
                meeting_id=self.meeting_id,
                output_filename=combined_audio_filename,
                pattern="chunk_*.wav"
            )
            
            if not os.path.exists(combined_audio_filepath):
                logger.error(f"[{self.meeting_id}] –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∞—É–¥–∏–æ—Ñ–∞–π–ª –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω: {combined_audio_filepath}")
                return
            
            dialog = "\n".join(
                f"[{seg['start']} - {seg['end']}] {seg['text']}"
                for seg in self.all_segments
            )
            print("–ò—Ç–æ–≥–æ–≤—ã–π –¥–∏–∞–ª–æ–≥:\n", dialog)

            cleaned_dialogue = "\n".join(seg['text'] for seg in self.all_segments)

            # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
            logger.info(f"[{self.meeting_id}] –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—é–º–µ...")
            summary_text = get_summary_response(cleaned_dialogue)
            print(f"–≠—Ç–æ –≤—ã–≤–æ–¥ summary: \n{summary_text}")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            logger.info(f"[{self.meeting_id}] –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞...")
            title_text = get_title_response(cleaned_dialogue)
            print(f"–≠—Ç–æ –≤—ã–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞: \n{title_text}")
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –≤–Ω–µ—à–Ω–∏–π —Å–µ—Ä–≤–µ—Ä
            self._send_results_to_backend(dialog, summary_text, title_text)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ
            # summary_filename = f"summary_{self.meeting_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            # summary_filepath = self.summary_output_dir / summary_filename

        except Exception as e:
            logger.error(f"[{self.meeting_id}] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}", exc_info=True)
        finally:
            logger.info(f"[{self.meeting_id}] –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ—á–∞–Ω–∫–æ–≤
    def _save_chunk(self, audio_np):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞—É–¥–∏–æ-—á–∞–Ω–∫ –≤ —Ñ–∞–π–ª WAV."""
        if audio_np.size == 0:
            return
        filename = f'chunk_{datetime.now().strftime("%Y%m%d_%H%M%S")}_{uuid4().hex[:6]}.wav'
        file_path = self.output_dir / filename
        try:
            sf.write(file_path, audio_np, STREAM_SAMPLE_RATE)
            logger.info(f"üíæ –§—Ä–∞–≥–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename} (–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {len(audio_np)/STREAM_SAMPLE_RATE:.2f} —Å–µ–∫)")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∞—É–¥–∏–æ—Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞: {e}")

    # –§—É–Ω–∫—Ü–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –≤–Ω–µ—à–Ω–∏–π —Å–µ—Ä–≤–µ—Ä
    def _send_results_to_backend(self, full_text: str, summary: str, title: str):
        try:
            payload = {"meeting_id": self.meeting_id, "full_text": full_text, "summary": summary, "title": title}
            headers = {"X-Internal-Api-Key": "key", "Content-Type": "application/json"}

            backend_url = os.getenv('MAIN_BACKEND_URL', 'https://maryrose.by')
            
            url = f"{backend_url}/meetings/internal/result"
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            logger.info(f"[{self.session_id}] ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è meeting_id {self.meeting_id} —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã.")
        except Exception as e:
            logger.error(f"[{self.session_id}] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ Main Backend: {e}")

    # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–æ—Ç–∞
    def stop(self):
        if not self.is_running.is_set():
            return
        
        logger.info(f"[{self.session_id}] –ü–æ–ª—É—á–µ–Ω–∞ –∫–æ–º–∞–Ω–¥–∞ –Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ...")
        self.is_running.clear()
        self.processor_thread.join()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        post_processing_thread = threading.Thread(target=self._perform_post_processing)
        post_processing_thread.daemon = False
        post_processing_thread.start()
        
        logger.info(f"[{self.session_id}] –°–µ—Å—Å–∏—è —Å —Å–∞–π—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –∑–∞–ø—É—â–µ–Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞.") 