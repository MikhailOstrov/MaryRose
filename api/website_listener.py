import os
import queue
import threading
import logging
from datetime import datetime
from uuid import uuid4

import numpy as np
import torch
from scipy.io.wavfile import write
import requests

from config.config import (
    STREAM_SAMPLE_RATE,
    MEET_FRAME_DURATION_MS,
    MEET_AUDIO_CHUNKS_DIR,
    SUMMARY_OUTPUT_DIR,
)

from handlers.ollama_handler import get_mary_response, get_summary_response, get_title_response
from handlers.diarization_handler import run_diarization, process_rttm_and_transcribe
from api.utils import combine_audio_chunks
from config.load_models import vad_model, asr_model
from config.config import SILENCE_THRESHOLD_FRAMES

logger = logging.getLogger(__name__)

class WebsiteListenerBot:

     # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∫–ª–∞—Å—Å–∞
    def __init__(self, session_id: str, meeting_id: int):

        self.session_id = session_id # ID –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–µ—Å—Å–∏–∏
        self.meeting_id = meeting_id # ID –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–µ—Å—Å–∏–∏
        self.audio_queue = queue.Queue() # –î–ª—è –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫–∞

        self.is_running = threading.Event()
        self.is_running.set()

        self.vad = vad_model # VAD-–º–æ–¥–µ–ª—å (from config.load_models import vad_model)
        self.asr_model = asr_model # Whisper (from config.load_models import asr_model)

        self.frame_size = int(STREAM_SAMPLE_RATE * (MEET_FRAME_DURATION_MS / 1000) * 2)
        self.silent_frames_threshold = SILENCE_THRESHOLD_FRAMES

        self.output_dir = MEET_AUDIO_CHUNKS_DIR / self.session_id
        os.makedirs(self.output_dir, exist_ok=True)
        logger.info(f"[{self.session_id}] –ê—É–¥–∏–æ—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å–µ—Å—Å–∏–∏ (meet_id: {self.meeting_id}) –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –≤: '{self.output_dir}'")

        self.processor_thread = threading.Thread(target=self._process_audio_stream)
        self.processor_thread.daemon = True
        self.processor_thread.start()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —á–∞–Ω–∫
    def feed_audio_chunk(self, chunk: bytes):
        if self.is_running.is_set():
            self.audio_queue.put(chunk)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫–∞ -- —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è -- –æ—Ç–≤–µ—Ç (–µ—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω —Ç—Ä–∏–≥–≥–µ—Ä)
    def _process_audio_stream(self):
        threading.current_thread().name = f'VADProcessor-{self.meeting_id}'
        logger.info(f"[{self.meeting_id}] –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä VAD –∑–∞–ø—É—â–µ–Ω —Å –º–æ–¥–µ–ª—å—é Silero.")

        vad_buffer = None
        VAD_CHUNK_SIZE = 512
        speech_buffer_for_asr = []
        is_speaking = False
        silent_frames_after_speech = 0
        
        TRIGGER_WORD = "–º—ç—Ä–∏"

        while self.is_running.is_set():
            try:

                audio_frame_bytes = self.audio_queue.get(timeout=1)
                if not audio_frame_bytes:
                    continue

                audio_np = np.frombuffer(audio_frame_bytes, dtype=np.int16).astype(np.float32) / 32768.0
                new_audio_tensor = torch.from_numpy(audio_np)

                if vad_buffer is None:
                    vad_buffer = new_audio_tensor
                else:
                    vad_buffer = torch.cat([vad_buffer, new_audio_tensor])

                while vad_buffer is not None and vad_buffer.shape[0] >= VAD_CHUNK_SIZE:
                    
                    chunk_to_process = vad_buffer[:VAD_CHUNK_SIZE]
                    vad_buffer = vad_buffer[VAD_CHUNK_SIZE:]
                    speech_prob = self.vad(chunk_to_process, STREAM_SAMPLE_RATE).item()
                    
                    if speech_prob > 0.3:
                        if not is_speaking:
                            logger.info(f"[{self.meeting_id}] –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–∞—á–∞–ª–æ —Ä–µ—á–∏.")
                            is_speaking = True
                        
                        speech_buffer_for_asr.append(chunk_to_process.numpy())
                        silent_frames_after_speech = 0
                    else:
                        if is_speaking:
                            
                            silent_frames_after_speech += 1
                            
                            if silent_frames_after_speech > self.silent_frames_threshold:
                                logger.info(f"[{self.meeting_id}] –û–±–Ω–∞—Ä—É–∂–µ–Ω –∫–æ–Ω–µ—Ü —Ñ—Ä–∞–∑—ã.")
                                is_speaking = False
                                silent_frames_after_speech = 0
                                
                                if speech_buffer_for_asr:
                                    full_audio_np = np.concatenate(speech_buffer_for_asr)
                                    speech_buffer_for_asr = []
                                    
                                    self._save_chunk(full_audio_np)

                                    segments, _ = self.asr_model.transcribe(full_audio_np, beam_size=5, language="ru")
                                    transcription = "".join([seg.text for seg in segments]).strip()
                                    
                                    if transcription:
                                        logger.info(f"[{self.meeting_id}] –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: '{transcription}'")
                                        
                                        if transcription.lower().lstrip().startswith(TRIGGER_WORD):
                                            logger.info(f"[{self.meeting_id}] –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å–ª–æ–≤–æ-—Ç—Ä–∏–≥–≥–µ—Ä. –û—Ç–ø—Ä–∞–≤–∫–∞ –∫–æ–º–∞–Ω–¥—ã...")
                                            response = get_mary_response(transcription)
                                            logger.info(f"[{self.meeting_id}] –û—Ç–≤–µ—Ç –æ—Ç –ú—ç—Ä–∏: {response}")
                
            except queue.Empty:
                if is_speaking and speech_buffer_for_asr:
                    logger.info(f"[{self.meeting_id}] –¢–∞–π–º-–∞—É—Ç, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à—É—é—Å—è —Ä–µ—á—å.")
                    is_speaking = False
                continue
            except Exception as e:
                logger.error(f"[{self.meeting_id}] –û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ VAD: {e}", exc_info=True)

    # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ—á–∞–Ω–∫–æ–≤ -- –∑–∞–ø—É—Å–∫ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–µ–π -- —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è -- –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ -- –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –≤–Ω–µ—à–Ω–∏–π —Å–µ—Ä–≤–µ—Ä
    def _perform_post_processing(self):

        threading.current_thread().name = f'PostProcessor-{self.meeting_id}'
        logger.info(f"[{self.meeting_id}] –ù–∞—á–∏–Ω–∞—é –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É...")

        try:
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ —á–∞–Ω–∫–æ–≤
            combined_audio_filename = f"combined_meeting_{self.meeting_id}.wav"
            combined_audio_filepath = self.output_dir / combined_audio_filename

            combine_audio_chunks(
                output_dir=self.output_dir,
                stream_sample_rate=STREAM_SAMPLE_RATE,
                meeting_id=self.meeting_id,
                output_filename=combined_audio_filename,
                pattern="chunk_*.wav"
            )
            
            if not os.path.exists(combined_audio_filepath):
                logger.error(f"[{self.meeting_id}] –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∞—É–¥–∏–æ—Ñ–∞–π–ª –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω: {combined_audio_filepath}")
                return
            
            # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è
            logger.info(f"[{self.meeting_id}] –ó–∞–ø—É—Å–∫ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏...")
            rttm_path = run_diarization(str(combined_audio_filepath), str(self.output_dir))
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ RTTM –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è (–≤–æ–∑–º–æ–∂–Ω–æ, —Å–ª–∏—è–Ω–∏–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–Ω–ª–∞–π–Ω STT)
            logger.info(f"[{self.meeting_id}] –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è...")
            dialogue_transcript = process_rttm_and_transcribe(rttm_path, str(combined_audio_filepath))
            print(f"–≠—Ç–æ –≤—ã–≤–æ–¥ –¥–∏–∞–ª–æ–≥–∞: \n{dialogue_transcript}")

            # –£–±–∏—Ä–∞–µ–º –º–µ—Ç–∫–∏ —Å–ø–∏–∫–µ—Ä–æ–≤, —á—Ç–æ –∏–∫—Å–ª—é—á–∏—Ç—å –∑–∞—Å–æ—Ä–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
            import re
            pattern = r"\[speaker_\d+\]:\s*"
            cleaned_dialogue = re.sub(pattern, "", dialogue_transcript)

            # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
            logger.info(f"[{self.meeting_id}] –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—é–º–µ...")
            summary_text = get_summary_response(cleaned_dialogue)
            print(f"–≠—Ç–æ –≤—ã–≤–æ–¥ summary: \n{summary_text}")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            logger.info(f"[{self.meeting_id}] –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞...")
            title_text = get_title_response(cleaned_dialogue)
            print(f"–≠—Ç–æ –≤—ã–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞: \n{title_text}")
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –≤–Ω–µ—à–Ω–∏–π —Å–µ—Ä–≤–µ—Ä
            self._send_results_to_backend(dialogue_transcript, summary_text, title_text, 30)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ
            # summary_filename = f"summary_{self.meeting_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            # summary_filepath = self.summary_output_dir / summary_filename

        except Exception as e:
            logger.error(f"[{self.meeting_id}] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}", exc_info=True)
        finally:
            logger.info(f"[{self.meeting_id}] –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ—á–∞–Ω–∫–æ–≤
    def _save_chunk(self, audio_np):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞—É–¥–∏–æ-—á–∞–Ω–∫ –≤ —Ñ–∞–π–ª WAV."""
        if audio_np.size == 0:
            return
        filename = f'chunk_{datetime.now().strftime("%Y%m%d_%H%M%S")}_{uuid4().hex[:6]}.wav'
        file_path = self.output_dir / filename
        try:
            write(str(file_path), STREAM_SAMPLE_RATE, audio_np)
            logger.info(f"üíæ –§—Ä–∞–≥–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename} (–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {len(audio_np)/STREAM_SAMPLE_RATE:.2f} —Å–µ–∫)")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∞—É–¥–∏–æ—Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞: {e}")

    # –§—É–Ω–∫—Ü–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –≤–Ω–µ—à–Ω–∏–π —Å–µ—Ä–≤–µ—Ä
    def _send_results_to_backend(self, full_text: str, summary: str, title: str, meeting_elapsed_sec: int):
        try:
            payload = {
                "meeting_id": self.meeting_id,
                "full_text": full_text,
                "summary": summary,
                "title": title,
                "meeting_elapsed_sec": meeting_elapsed_sec
            }
            headers = {"X-Internal-Api-Key": "key", "Content-Type": "application/json"}

            backend_url = os.getenv('MAIN_BACKEND_URL', 'https://maryrose.by')
            
            url = f"{backend_url}/meetings/internal/result"
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            logger.info(f"[{self.session_id}] ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è meeting_id {self.meeting_id} —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã.")
        except Exception as e:
            logger.error(f"[{self.session_id}] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ Main Backend: {e}")

    # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–æ—Ç–∞
    def stop(self):
        if not self.is_running.is_set():
            return
        
        self.is_running.clear()
        self.processor_thread.join()
        
        post_processing_thread = threading.Thread(target=self._perform_post_processing)
        post_processing_thread.daemon = False
        post_processing_thread.start()
        
        logger.info(f"[{self.session_id}] –°–µ—Å—Å–∏—è —Å —Å–∞–π—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –∑–∞–ø—É—â–µ–Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞.") 